{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import google.generativeai as genai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ROOT = '../'\n",
    "sys.path.append(ROOT)  # Add the root folder to the sys.path\n",
    "\n",
    "# Import the modules\n",
    "from config import *\n",
    "from utils.config import *\n",
    "from utils.utils import *\n",
    "from utils.gemini_utils import *\n",
    "\n",
    "# Reload the configuration\n",
    "from importlib import reload\n",
    "reload(sys.modules['config'])\n",
    "reload(sys.modules['utils.config'])\n",
    "reload(sys.modules['utils.utils'])\n",
    "reload(sys.modules['utils.gemini_utils'])\n",
    "\n",
    "# Import the reloaded modules\n",
    "from config import *\n",
    "from utils.config import *\n",
    "from utils.utils import *\n",
    "from utils.gemini_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import reddit credentials from twitter.json\n",
    "with open(os.path.join(ROOT, 'secrets/gemini.json')) as file:\n",
    "    creds = json.load(file)  \n",
    "\n",
    "# Select the Google API key\n",
    "google_api_key = creds['GOOGLE_API_KEY_2']\n",
    "\n",
    "# Set up the API key\n",
    "genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini model configurations\n",
    "generation_config, safety_settings = gemini_configurations()\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=MODEL_NAME,\n",
    "  safety_settings=safety_settings,\n",
    "  generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths\n",
    "DATASET_TYPE = \"daily\" # \"daily\" | \"hourly\"\n",
    "ANNOTATED_DATASET_NAME = DATASET_TYPE + \"_\" + MODEL_NAME + \"_opinion.csv\"\n",
    "\n",
    "# Set the paths\n",
    "ORIGINAL_DATASET_PATH = os.path.join(ROOT, MERGED_DATASET_PATH, \"merged_\" + DATASET_TYPE + \".csv\")\n",
    "OPINION_DATASET_PATH = os.path.join(ROOT, ANNOTATED_DATASET_PATH, ANNOTATED_DATASET_NAME)\n",
    "OUTPUT_DATASET_PATH = os.path.join(ROOT, ANNOTATED_DATASET_PATH, \"merged_\" + ANNOTATED_DATASET_NAME)\n",
    "OUTPUT_NO_TEXT_DATASET_PATH = os.path.join(ROOT, ANNOTATED_DATASET_PATH, \"merged_no_text_\" + ANNOTATED_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read daily dataset from the file\n",
    "original_dataset = pd.read_csv(ORIGINAL_DATASET_PATH)\n",
    "original_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_rows = []\n",
    "empty_rows = []\n",
    "\n",
    "# Check if the dataset exists\n",
    "if os.path.exists(OPINION_DATASET_PATH):\n",
    "    print(f\"Loading the {OPINION_DATASET_PATH} dataset...\")\n",
    "    # Load the opinion_df dataset\n",
    "    opinion_df = pd.read_csv(OPINION_DATASET_PATH)\n",
    "\n",
    "    # Sobstitute the NaN values with None\n",
    "    opinion_df.fillna('None', inplace=True)\n",
    "\n",
    "    # Select the rows that are and are not None\n",
    "    non_empty_rows = opinion_df[opinion_df['reasoning_text'] != 'None']\n",
    "    empty_rows = opinion_df[opinion_df['reasoning_text'] == 'None']\n",
    "\n",
    "    # Display the number of rows that are not None\n",
    "    print(f\"Number of rows that are not None: {non_empty_rows.shape[0]}\")\n",
    "    print(f\"Number of rows that are None: {empty_rows.shape[0]}\")\n",
    "else:\n",
    "    print(f\"Creating the {OPINION_DATASET_PATH} dataset...\")\n",
    "    # Create a new dataset with row_index, reasoning_text and sentiment_class columns starting from the merged_daily dataset\n",
    "    # Copy the index from the merged_daily dataset to the new dataset\n",
    "    opinion_df = original_dataset.copy()\n",
    "    # Drop the columns from the new dataset except the index\n",
    "    opinion_df.drop(columns=original_dataset.columns, inplace=True)\n",
    "    # Add the reasoning_text and sentiment_class columns to the new dataset\n",
    "    opinion_df['reasoning_text'] = 'None'\n",
    "    opinion_df['sentiment_class'] = 'None'\n",
    "    opinion_df['action_class'] = 'None'\n",
    "    opinion_df['action_score'] = 'None'\n",
    "opinion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the func_kwargs path\n",
    "func_kwargs_path = os.path.join(ROOT, ANNOTATED_DATASET_PATH, FUNC_KWARGS_FOLDER_NAME, FUNC_KWARGS_FOLDER_NAME + \"_\" + MODEL_NAME + \".json\")\n",
    "\n",
    "# Check if it is a test or not\n",
    "TEST = False\n",
    "\n",
    "# Check if the func_kwargs file exists\n",
    "if os.path.exists(func_kwargs_path):\n",
    "    print(f\"Loading the {func_kwargs_path} file...\")\n",
    "    # Load the func_kwargs file\n",
    "    with open(func_kwargs_path, 'r') as f:\n",
    "        func_kwargs = json.load(f)\n",
    "else:\n",
    "    print(f\"Creating the {func_kwargs_path} file...\")\n",
    "    # Create a new func_kwargs\n",
    "    # For each row in the dataset, populate the func_kwargs list with the input text and the index of each row\n",
    "    func_kwargs = populate_func_kwargs(\n",
    "        model_name=MODEL_NAME, \n",
    "        merged_dataset=original_dataset, \n",
    "        opinion_dataset=opinion_df, \n",
    "        max_tokens=INPUT_TOKENS, \n",
    "        instructions=INSTRUCTIONS, \n",
    "        model_tokenizer=None, # No tokenizer is needed\n",
    "        test=TEST,\n",
    "        )\n",
    "\n",
    "    if not TEST:\n",
    "        # Save the func_kwargs dictionary to the file\n",
    "        with open(func_kwargs_path, 'w') as f:\n",
    "            json.dump(func_kwargs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_kwargs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the queries that are not annotated\n",
    "func_kwargs = [query for query in func_kwargs if opinion_df.loc[query['index'], 'reasoning_text'] == 'None']\n",
    "\n",
    "# Conunt the number of queries that are not annotated\n",
    "print(f\"Number of queries that are already annotated: {len(opinion_df) - len(func_kwargs)}\")\n",
    "print(f\"Number of queries that are not annotated: {len(func_kwargs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Gemini API using RateNinja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_api(index, input_text):\n",
    "    try:\n",
    "        # Generate the reasoning and sentiment\n",
    "        response = model.generate_content(input_text)\n",
    "        response_json = ast.literal_eval(response.text)\n",
    "\n",
    "        # Check the response\n",
    "        reasoning_text, sentiment_class, action_class, action_score = check_response(response_json)\n",
    "\n",
    "        # Update the sentiment dataset\n",
    "        opinion_df.loc[index, 'reasoning_text'] = reasoning_text\n",
    "        opinion_df.loc[index, 'sentiment_class'] = sentiment_class\n",
    "        opinion_df.loc[index, 'action_class'] = action_class\n",
    "        opinion_df.loc[index, 'action_score'] = action_score\n",
    "\n",
    "        # Save temporary results\n",
    "        opinion_df.to_csv(os.path.join(ROOT, ANNOTATED_DATASET_PATH, ANNOTATED_DATASET_NAME), index=False)\n",
    "\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise Exception(f\"Error: {e} at index {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the API call\n",
    "results, errors = RATENINJA(call_gemini_api, func_args=None, func_kwargs=func_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_empty_rows = empty_rows.copy()\n",
    "previous_non_empty_rows = non_empty_rows.copy()\n",
    "\n",
    "# Show the previous number of empty and non-empty rows\n",
    "print(f\"Previous number of empty rows: {len(previous_empty_rows)}\")\n",
    "print(f\"Previous number of non-empty rows: {len(previous_non_empty_rows)}\")\n",
    "\n",
    "# Select the empty and non-empty rows\n",
    "empty_rows = opinion_df[(opinion_df['reasoning_text'] == 'None') | (opinion_df['sentiment_class'] == 'None') | (opinion_df['action_class'] == 'None') | (opinion_df['action_score'] == 'None')]\n",
    "non_empty_rows = opinion_df[(opinion_df['reasoning_text'] != 'None') & (opinion_df['sentiment_class'] != 'None') & (opinion_df['action_class'] != 'None') & (opinion_df['action_score'] != 'None')]\n",
    "\n",
    "# Show the new number of empty and non-empty rows\n",
    "print(f\"New number of empty rows: {empty_rows.shape[0]}\")\n",
    "print(f\"New number of non-empty rows: {non_empty_rows.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values\n",
    "print(f\"Unique sentiment_class values: {opinion_df['sentiment_class'].unique()}\")\n",
    "print(f\"Unique action_class values: {opinion_df['action_class'].unique()}\")\n",
    "print(f\"Unique action_score values: {opinion_df['action_score'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the columns\n",
    "opinion_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append opinion_df to the original dataset\n",
    "original_dataset_with_opinion = pd.concat([original_dataset, opinion_df], axis=1)\n",
    "original_dataset_with_opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the original one without text (i.e., except cointelegrap, reddit, reasoning_text columns)\n",
    "original_dataset_without_text = original_dataset_with_opinion.drop(columns=['cointelegraph', 'bitcoin_news', 'reddit', 'reasoning_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_without_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the daily opinion dataset\n",
    "opinion_df.to_csv(os.path.join(ROOT, ANNOTATED_DATASET_PATH, ANNOTATED_DATASET_NAME), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the daily dataset with no text\n",
    "original_dataset_without_text.to_csv(OUTPUT_NO_TEXT_DATASET_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the daily merged dataset\n",
    "original_dataset_with_opinion.to_csv(OUTPUT_DATASET_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
