{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ROOT = '../'\n",
    "sys.path.append(ROOT)  # Add the root folder to the sys.path\n",
    "\n",
    "# Import the modules\n",
    "from config import *\n",
    "\n",
    "# Reload the configuration\n",
    "from importlib import reload\n",
    "reload(sys.modules['config'])\n",
    "\n",
    "# Import the reloaded modules\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Cointelegraph news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Define the parameters\n",
    "start = 0\n",
    "MAX_LENGHT = 4000\n",
    "\n",
    "# Parameters\n",
    "short = \"en\"\n",
    "slug = \"bitcoin\"\n",
    "order = \"postPublishedTime\"\n",
    "offset = str(start) # From the last news published\n",
    "length = str(MAX_LENGHT) \n",
    "\n",
    "data = []\n",
    "while True:\n",
    "    try:\n",
    "        print(f\"Fetching data from {start} to {start + MAX_LENGHT}\")\n",
    "        # Define the curl command with parameters\n",
    "        curl_command = [\n",
    "            \"curl\",\n",
    "            \"https://conpletus.cointelegraph.com/v1/\",\n",
    "            \"--compressed\",\n",
    "            \"-X\", \"POST\",\n",
    "            \"-H\", \"Accept-Encoding: gzip, deflate\",\n",
    "            \"-H\", \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\",\n",
    "            \"-H\", \"Accept: application/graphql-response+json, application/graphql+json, application/json, text/event-stream, multipart/mixed\",\n",
    "            \"-H\", \"Accept-Language: en-US,en;q=0.5\",\n",
    "            \"-H\", \"Accept-Encoding: gzip, deflate, br, zstd\",\n",
    "            \"-H\", \"Referer: https://cointelegraph.com/\",\n",
    "            \"-H\", \"baggage: \",\n",
    "            \"-H\", \"sentry-trace: \",\n",
    "            \"-H\", \"content-type: application/json\",\n",
    "            \"-H\", \"Origin: https://cointelegraph.com\",\n",
    "            \"-H\", \"Connection: keep-alive\",\n",
    "            \"-H\", \"Sec-Fetch-Dest: empty\",\n",
    "            \"-H\", \"Sec-Fetch-Mode: cors\",\n",
    "            \"-H\", \"Sec-Fetch-Site: same-site\",\n",
    "            \"-H\", \"Priority: u=4\",\n",
    "            \"-H\", \"TE: trailers\",\n",
    "            \"--data-raw\", '{\"operationName\":\"TagPageQuery\",\"query\":\"query TagPageQuery($short: String, $slug: String!, $order: String, $offset: Int!, $length: Int!) {\\\\n  locale(short: $short) {\\\\n    tag(slug: $slug) {\\\\n      id\\\\n      slug\\\\n      avatar\\\\n      createdAt\\\\n      updatedAt\\\\n      redirectRelativeUrl\\\\n      alternates {\\\\n        short\\\\n        domain\\\\n        id\\\\n        code\\\\n        __typename\\\\n      }\\\\n      tagTranslates {\\\\n        id\\\\n        title\\\\n        metaTitle\\\\n        pageTitle\\\\n        description\\\\n        metaDescription\\\\n        keywords\\\\n        __typename\\\\n      }\\\\n      posts(order: $order, offset: $offset, length: $length) {\\\\n        data {\\\\n          id\\\\n          slug\\\\n          views\\\\n          postTranslate {\\\\n            id\\\\n            title\\\\n            avatar\\\\n            published\\\\n            publishedHumanFormat\\\\n            leadText\\\\n            author {\\\\n              id\\\\n              slug\\\\n              authorTranslates {\\\\n                id\\\\n                name\\\\n                __typename\\\\n              }\\\\n              __typename\\\\n            }\\\\n            __typename\\\\n          }\\\\n          category {\\\\n            id\\\\n            slug\\\\n            __typename\\\\n          }\\\\n          author {\\\\n            id\\\\n            slug\\\\n            authorTranslates {\\\\n              id\\\\n              name\\\\n              __typename\\\\n            }\\\\n            __typename\\\\n          }\\\\n          postBadge {\\\\n            id\\\\n            label\\\\n            postBadgeTranslates {\\\\n              id\\\\n              title\\\\n              __typename\\\\n            }\\\\n            __typename\\\\n          }\\\\n          showShares\\\\n          showStats\\\\n          __typename\\\\n        }\\\\n        postsCount\\\\n        __typename\\\\n      }\\\\n      __typename\\\\n    }\\\\n    __typename\\\\n  }\\\\n}\",\"variables\":{\"cacheTimeInMS\":300000,\"length\":'+length+',\"offset\":'+offset+',\"order\":\"'+order+'\",\"short\":\"'+short+'\",\"slug\":\"'+slug+'\"}}',\n",
    "        ]\n",
    "\n",
    "        # Execute the curl command\n",
    "        result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "\n",
    "        # Turn result.stdout into json\n",
    "        data.append(json.loads(result.stdout))\n",
    "\n",
    "        print(f\"Data fetched from {start} to {start + MAX_LENGHT}\")\n",
    "\n",
    "        # Save the last date\n",
    "        last_date = data[-1]['data']['locale']['tag']['posts']['data'][-1]['postTranslate']['published']\n",
    "\n",
    "        # Convert into YYYY-MM-DD format\n",
    "        last_date = dt.strptime(last_date, \"%Y-%m-%dT%H:%M:%S%z\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        print(f\"last post published at {last_date}\")\n",
    "\n",
    "        # Check if the last date is before the start date\n",
    "        if last_date < START_DATE:\n",
    "            print(f\"Reached the start date {START_DATE}\")\n",
    "            break \n",
    "\n",
    "        # Increase the offset\n",
    "        start += MAX_LENGHT\n",
    "        offset = str(start)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Reached the end of the data\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a list all the news\n",
    "news = []\n",
    "for d in data:\n",
    "    news.extend(d['data']['locale']['tag']['posts']['data'])\n",
    "\n",
    "print(len(news))\n",
    "\n",
    "print(news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List to hold dictionaries\n",
    "news_list = []\n",
    "\n",
    "# Loop through news items and create dictionaries\n",
    "for new in news:\n",
    "    news_dict = {\n",
    "        \"id\": new[\"id\"],\n",
    "        \"slug\": new[\"slug\"],\n",
    "        \"views\": new[\"views\"],\n",
    "        \"title\": new[\"postTranslate\"][\"title\"],\n",
    "        \"published\": new[\"postTranslate\"][\"published\"],\n",
    "        \"leadtext\": new[\"postTranslate\"][\"leadText\"]\n",
    "    }\n",
    "    news_list.append(news_dict)\n",
    "\n",
    "# Create DataFrame from list of dictionaries\n",
    "df = pd.DataFrame(news_list)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"2013-08-05T10:30:00+01:00\" to \"2024-07-17 15:30:28\"\n",
    "df['published'] = df['published'].apply(lambda x: dt.strptime(x, '%Y-%m-%dT%H:%M:%S%z').strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "output_file = os.path.join(ROOT, NEWS_DATASET_PATH, 'cointelegraph_raw.csv')\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ADDON] Get \"body\" field of Coitelegraph news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "input_file = os.path.join(ROOT, NEWS_DATASET_PATH, 'cointelegraph_raw.csv')\n",
    "df = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column \"body\" to cointelegraph_with_body_with_sentiment between \"leadtext\" and \"sentiment\"\n",
    "cointelegraph_with_body = df.copy()\n",
    "cointelegraph_with_body.insert(6, \"body\", 'None')\n",
    "cointelegraph_with_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text_to_remove(body):\n",
    "    # Extract all the <p ...>...</p> tags that contains <strong>...</strong> with <em>Related:</em> inside them\n",
    "    # Find all the <p> tags\n",
    "    text_to_remove = []\n",
    "    p_tags = body.find_all('p')\n",
    "    for p in p_tags:\n",
    "        # Find all the <strong> tags inside the <p> tag\n",
    "        strong_tags = p.find_all('strong')\n",
    "        for strong in strong_tags:\n",
    "            # Find all the <em> tags inside the <strong> tag\n",
    "            em_tags = strong.find_all('em')\n",
    "            for em in em_tags:\n",
    "                # Check if the <em> tag contains the text \"Related:\"\n",
    "                if \"Related:\" in em.get_text():\n",
    "                    text_to_remove.append(p)\n",
    "                    \n",
    "    # Extract all the <p ...>...</p> tags that contains <strong>...</strong> with <em>Magazine:</em> inside them\n",
    "    p_tags = body.find_all('p')\n",
    "    for p in p_tags:\n",
    "        # Find all the <strong> tags inside the <p> tag\n",
    "        strong_tags = p.find_all('strong')\n",
    "        for strong in strong_tags:\n",
    "            # Find all the <em> tags inside the <strong> tag\n",
    "            em_tags = strong.find_all('em')\n",
    "            for em in em_tags:\n",
    "                # Check if the <em> tag contains the text \"Magazine:\"\n",
    "                if \"Magazine:\" in em.get_text():\n",
    "                    text_to_remove.append(p)\n",
    "\n",
    "    # Extract all the <p ...>...</p> tags that contains <strong>...</strong> with <em>Recent:</em> inside them\n",
    "    p_tags = body.find_all('p')\n",
    "    for p in p_tags:\n",
    "        # Find all the <strong> tags inside the <p> tag\n",
    "        strong_tags = p.find_all('strong')\n",
    "        for strong in strong_tags:\n",
    "            # Find all the <em> tags inside the <strong> tag\n",
    "            em_tags = strong.find_all('em')\n",
    "            for em in em_tags:\n",
    "                # Check if the <em> tag contains the text \"Recent:\"\n",
    "                if \"Recent:\" in em.get_text():\n",
    "                    text_to_remove.append(p)\n",
    "\n",
    "    return text_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def retrieve_html(url):\n",
    "    soup = None\n",
    "\n",
    "    # With Charles Proxy/HTTP Toolkit\n",
    "    headers = {\n",
    "        \"Host\": \"cointelegraph.com\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:129.0) Gecko/20100101 Firefox/129.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Referer\": \"https://cointelegraph.com/tags/bitcoin\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Priority\": \"u=0, i\",\n",
    "        \"TE\": \"trailers\"\n",
    "    }\n",
    "\n",
    "    # # Retrieve the cookie from the secrets/cookies.json file\n",
    "    # with open(os.path.join(ROOT, 'secrets/cookies.json')) as file:\n",
    "    #     cookie = json.load(file)['cookie']\n",
    "\n",
    "    # # With Requestly Proxy\n",
    "    # headers = {\n",
    "    #     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:130.0) Gecko/20100101 Firefox/130.0\",\n",
    "    #     \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\",\n",
    "    #     \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    #     \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "    #     \"Connection\": \"keep-alive\",\n",
    "    #     \"Cookie\": cookie,\n",
    "    #     \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    #     \"Sec-Fetch-Dest\": \"document\",\n",
    "    #     \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    #     \"Sec-Fetch-Site\": \"none\",\n",
    "    #     \"Sec-Fetch-User\": \"?1\",\n",
    "    #     \"Priority\": \"u=0, i\"\n",
    "    # }\n",
    "\n",
    "    # Get the body of the article from the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "      \n",
    "    # Retry if the status code is not 200\n",
    "    if response.status_code != 200:\n",
    "        count = 0\n",
    "        while response.status_code != 200 and count < 5:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            count += 1\n",
    "\n",
    "    else:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cointelegraph_body(url, cus_class):\n",
    "    import requests\n",
    "\n",
    "    try:\n",
    "        # Get the body of the article from the URL\n",
    "        soup = retrieve_html(url)\n",
    "\n",
    "        if soup == None:\n",
    "            print(f\"Failed to get the content of the article: {url}\")\n",
    "            return 'None'\n",
    "\n",
    "        # Find the body of the article\n",
    "        body = soup.find('div', class_=cus_class)\n",
    "\n",
    "        # Clean the body\n",
    "        text_to_remove = find_text_to_remove(body)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Failed to find the body of the article: {url}\")\n",
    "        return 'None'\n",
    "\n",
    "    # Get the text of the body\n",
    "    body_text = body.get_text()\n",
    "\n",
    "    # Remove the text that contains \"Related:\"\n",
    "    for text in text_to_remove:\n",
    "        body_text = body_text.replace(text.get_text(), '')\n",
    "    \n",
    "    if body_text == '':      \n",
    "        print(f\"Failed to find the body of the article in the URL: {url}\")\n",
    "        return 'None'\n",
    "    return body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "# For each news open cointelegrap.com/news/{slug} and save the content\n",
    "for index, row in tqdm(cointelegraph_with_body.iterrows(), total=len(cointelegraph_with_body)):\n",
    "    try:\n",
    "        # Get the slug\n",
    "        slug = row['slug']\n",
    "\n",
    "        # Open the URL\n",
    "        url = f\"https://cointelegraph.com/news/{slug}\"\n",
    "        \n",
    "        # Get the body\n",
    "        cus_class = 'post-content relative'\n",
    "        body = get_cointelegraph_body(url, cus_class)\n",
    "\n",
    "        # Save the body into the dataset\n",
    "        cointelegraph_with_body.at[index, 'body'] = body\n",
    "    except Exception as e:\n",
    "       print(e)\n",
    "       cointelegraph_with_body.at[index, 'body'] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cointelegraph_with_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a CSV file\n",
    "output_file = os.path.join(ROOT, NEWS_DATASET_PATH, 'cointelegraph_with_body.csv')\n",
    "cointelegraph_with_body.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ADDON] Retrieve 'None' body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "cointelegraph_with_body = pd.read_csv(os.path.join(ROOT, NEWS_DATASET_PATH, 'cointelegraph_with_body.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for None values\n",
    "cointelegraph_with_null_body = cointelegraph_with_body[cointelegraph_with_body['body'].isnull()]\n",
    "cointelegraph_with_null_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text_to_remove(body):\n",
    "    # Extract all the <p ...>...</p> tags that contains <strong>...</strong> with <em>Related:</em> inside them\n",
    "    # Find all the <p> tags\n",
    "    text_to_remove = []\n",
    "    p_tags = body.find_all('p')\n",
    "    for p in p_tags:\n",
    "        # Find all the <strong> tags inside the <p> tag\n",
    "        strong_tags = p.find_all('strong')\n",
    "        for strong in strong_tags:\n",
    "            # Find all the <em> tags inside the <strong> tag\n",
    "            em_tags = strong.find_all('em')\n",
    "            for em in em_tags:\n",
    "                # Check if the <em> tag contains the text \"Related:\"\n",
    "                if \"Related:\" in em.get_text():\n",
    "                    text_to_remove.append(p)\n",
    "                    \n",
    "    # Extract all the <p ...>...</p> tags that contains <strong>...</strong> with <em>Magazine:</em> inside them\n",
    "    p_tags = body.find_all('p')\n",
    "    for p in p_tags:\n",
    "        # Find all the <strong> tags inside the <p> tag\n",
    "        strong_tags = p.find_all('strong')\n",
    "        for strong in strong_tags:\n",
    "            # Find all the <em> tags inside the <strong> tag\n",
    "            em_tags = strong.find_all('em')\n",
    "            for em in em_tags:\n",
    "                # Check if the <em> tag contains the text \"Magazine:\"\n",
    "                if \"Magazine:\" in em.get_text():\n",
    "                    text_to_remove.append(p)\n",
    "\n",
    "    # Extract all the <p ...>...</p> tags that contains <strong>...</strong> with <em>Recent:</em> inside them\n",
    "    p_tags = body.find_all('p')\n",
    "    for p in p_tags:\n",
    "        # Find all the <strong> tags inside the <p> tag\n",
    "        strong_tags = p.find_all('strong')\n",
    "        for strong in strong_tags:\n",
    "            # Find all the <em> tags inside the <strong> tag\n",
    "            em_tags = strong.find_all('em')\n",
    "            for em in em_tags:\n",
    "                # Check if the <em> tag contains the text \"Recent:\"\n",
    "                if \"Recent:\" in em.get_text():\n",
    "                    text_to_remove.append(p)\n",
    "\n",
    "    return text_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cointelegraph_body(url, cus_class):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # Get the body of the article from the URL\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Retry if the status code is not 200\n",
    "    if response.status_code != 200:\n",
    "        count = 0\n",
    "        while response.status_code != 200 and count < 10:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            count += 1\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get the URL: {url}\")\n",
    "        return 'None'\n",
    "        \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the body of the article\n",
    "    body = soup.find('div', class_=cus_class)\n",
    "\n",
    "    try:\n",
    "        # Clean the body\n",
    "        text_to_remove = find_text_to_remove(body)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Failed to find the body of the article in the URL: {url}\")\n",
    "        return 'None'\n",
    "\n",
    "    # Get the text of the body\n",
    "    body_text = body.get_text()\n",
    "\n",
    "    # Remove the text that contains \"Related:\"\n",
    "    for text in text_to_remove:\n",
    "        body_text = body_text.replace(text.get_text(), '')\n",
    "    \n",
    "    if body_text == '':      \n",
    "        print(f\"Failed to find the body of the article in the URL: {url}\")\n",
    "\n",
    "        return 'None'\n",
    "    return body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each news open cointelegrap.com/news/{slug} and save the content\n",
    "for index, row in tqdm(cointelegraph_with_null_body.iterrows(), total=len(cointelegraph_with_null_body)):\n",
    "    try:\n",
    "        # Get the slug\n",
    "        slug = row['slug']\n",
    "\n",
    "        # Open the URL\n",
    "        url = f'https://www.cointelegraph.com/news/{slug}' # 'https://www.cointelegraph.com/news/{slug}' | 'https://www.cointelegraph.com/explained/{slug}'\n",
    "        \n",
    "        # Get the body\n",
    "        cus_class = 'post-content relative shadow-[0_2px_32px_0px_#00000014] bg-custom-gray-50' # 'post-content relative' | 'post-content relative shadow-[0_2px_32px_0px_#00000014] bg-custom-gray-50'\n",
    "        body = get_cointelegraph_body(url, cus_class)\n",
    "\n",
    "        # Save the body into the dataset\n",
    "        cointelegraph_with_null_body.at[index, 'body'] = body\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # Set the body to None\n",
    "        cointelegraph_with_null_body.at[index, 'body'] = 'None'\n",
    "\n",
    "    # Save the dataset to a CSV file\n",
    "    output_file = os.path.join(ROOT, NEWS_DATASET_PATH, 'cointelegraph_with_body.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cointelegraph_with_null_body with cointelegraph_with_body\n",
    "cointelegraph_with_body.update(cointelegraph_with_null_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for None values\n",
    "cointelegraph_with_null_body = cointelegraph_with_null_body[cointelegraph_with_null_body['body'].isnull()]\n",
    "cointelegraph_with_null_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the remaining rows with body equal to None\n",
    "cointelegraph_with_body.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cointelegraph_with_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a CSV file\n",
    "output_file = os.path.join(ROOT, NEWS_DATASET_PATH, 'cointelegraph_with_body.csv')\n",
    "cointelegraph_with_body.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate daily and hourly Cointelegraph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open cointelegraph dataset\n",
    "cointelegraph_with_body = pd.read_csv(os.path.join(ROOT, NEWS_DATASET_PATH, \"cointelegraph_with_body.csv\"))\n",
    "cointelegraph_with_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two temp dataset (daily and hourly) with just the timestamp column that start from START_DATE and end at END_DATE\n",
    "# Daily dataset has the format: 2024-03-16\n",
    "# Hourly dataset has the format: 2024-03-16 20:00:00\n",
    "\n",
    "# Define the start date\n",
    "start_date = cointelegraph_with_body['published'].min() # Default: START_DATE\n",
    "end_date = cointelegraph_with_body['published'].max() # Default: END_DATE\n",
    "\n",
    "cointelegraph_daily = pd.date_range(start=start_date, end=end_date, freq='D').to_frame(index=False, name='timestamp')\n",
    "cointelegraph_daily['timestamp'] = cointelegraph_daily['timestamp'].dt.date\n",
    "\n",
    "# start_date = cointelegraph_with_body['published'].min()+\" 00:00:00\" # Default: START_DATE\n",
    "\n",
    "# For the hourly dataset, generate 2 coluns: timestamp_begin and timestamp_end where timestamp_end is timestamp_begin + 1 hour\n",
    "# cointelegraph_hourly = pd.date_range(start=start_date, end=END_DATE, freq='h').to_frame(index=False, name='timestamp')\n",
    "# cointelegraph_hourly['timestamp_begin'] = cointelegraph_hourly['timestamp']\n",
    "# cointelegraph_hourly['timestamp_end'] = cointelegraph_hourly['timestamp'] + pd.Timedelta(hours=1)\n",
    "# cointelegraph_hourly.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# Turn the timestamp column into string\n",
    "cointelegraph_daily['timestamp'] = cointelegraph_daily['timestamp'].astype(str)\n",
    "# cointelegraph_hourly['timestamp_begin'] = cointelegraph_hourly['timestamp_begin'].astype(str)\n",
    "# cointelegraph_hourly['timestamp_end'] = cointelegraph_hourly['timestamp_end'].astype(str)\n",
    "\n",
    "# Add 'cointelegraph' column\n",
    "cointelegraph_daily['cointelegraph'] = None\n",
    "# cointelegraph_hourly['cointelegraph'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cointelegraph_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cointelegraph_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cointelegraph_daily\n",
    "cointelegraph_daily_copy = cointelegraph_daily.copy()\n",
    "\n",
    "# Iterate over the daily dataset\n",
    "for index, row in tqdm(cointelegraph_daily_copy.iterrows(), total=len(cointelegraph_daily_copy)):\n",
    "    # Get the timestamp\n",
    "    curr_timestamp = row['timestamp']\n",
    "    # Select the cointelegraph rows items that have been published during the curr_timestamp, save them as a list\n",
    "    # Example: if curr_timestamp is 2018-01-01, then select the news items that have been published from 2018-01-01 00:00:00 to 2018-01-01 23:59:59, news = [news1, news2, news3]\n",
    "    filtered_news = cointelegraph_with_body[(cointelegraph_with_body['published'].str.contains(curr_timestamp))]\n",
    "\n",
    "    if len(filtered_news) == 0:\n",
    "        cointelegraph_daily_copy.at[index, 'cointelegraph'] = str([])\n",
    "    else:\n",
    "        # Convert news dataframe to a list\n",
    "        filtered_news = str(filtered_news.values.tolist())\n",
    "        # Append the news list to the news column\n",
    "        cointelegraph_daily_copy.at[index, 'cointelegraph'] = filtered_news\n",
    "cointelegraph_daily_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NOT empty rows (different from []) in the cointelegraph column\n",
    "not_empty_rows = cointelegraph_daily_copy[cointelegraph_daily_copy['cointelegraph'] != '[]']\n",
    "print(\"Total number of NOT '[]' occurrences in the cointelegraph column:\", not_empty_rows.shape[0])\n",
    "not_empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example\n",
    "row = not_empty_rows['cointelegraph'][0]\n",
    "print(f\"Row has {len(row)} news\")\n",
    "print(f\"List of news: {row}\")\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of empty rows (equal to '[]') in the cointelegraph column\n",
    "empty_rows = cointelegraph_daily_copy[cointelegraph_daily_copy['cointelegraph'] == '[]']\n",
    "print(\"Total number of '[]' occurrences in the cointelegraph column:\", empty_rows.shape[0])\n",
    "empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the timestamp as the index\n",
    "cointelegraph_daily_copy = cointelegraph_daily_copy.set_index('timestamp', drop=False)\n",
    "cointelegraph_daily = cointelegraph_daily_copy\n",
    "cointelegraph_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Increase the figure size\n",
    "# plt.figure(figsize=(24, 8))\n",
    "\n",
    "# # Plot the number of news items per day\n",
    "# plt.plot(cointelegraph_daily['timestamp'], cointelegraph_daily['cointelegraph'].str.count('\\[[^\\]]'))\n",
    "\n",
    "# # Set x intervals\n",
    "# plt.xticks(cointelegraph_daily['timestamp'][::60], rotation=45)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate cointelegraph_hourly\n",
    "# cointelegraph_hourly_copy = cointelegraph_hourly.copy()\n",
    "\n",
    "# # Iterate over the hourly dataset\n",
    "# for index, row in tqdm(cointelegraph_hourly_copy.iterrows(), total=len(cointelegraph_hourly_copy)):\n",
    "#     # Get the timestamp\n",
    "#     timestamp_begin = row['timestamp_begin']\n",
    "#     timestamp_end = row['timestamp_end']\n",
    "#     # Select the cointelegraph rows items that have been published between timestamp_begin and timestamp_end, save them as a list\n",
    "#     # Example: if timestamp_begin is 2018-01-01 00:00:00 and timestamp_end is 2018-01-01 01:00:00\n",
    "#     # select the news items that have been published from 2018-01-01 00:00:00 to 2018-01-01 00:59:59\n",
    "#     # news = [news1, news2, news3]\n",
    "#     filtered_news = cointelegraph_with_body[\n",
    "#         (cointelegraph_with_body['published'] >= timestamp_begin) & \n",
    "#         (cointelegraph_with_body['published'] < timestamp_end)\n",
    "#     ]\n",
    "\n",
    "#     if len(filtered_news) == 0:\n",
    "#         cointelegraph_hourly_copy.at[index, 'cointelegraph'] = str([])\n",
    "#     else:\n",
    "#         # Convert news dataframe to a list\n",
    "#         filtered_news = str(filtered_news.values.tolist())\n",
    "#         # Append the news list to the news column\n",
    "#         cointelegraph_hourly_copy.at[index, 'cointelegraph'] = filtered_news\n",
    "# cointelegraph_hourly_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of NOT empty rows (different from []) in the cointelegraph column\n",
    "# not_empty_rows = cointelegraph_hourly_copy[cointelegraph_hourly_copy['cointelegraph'] != '[]']\n",
    "# print(\"Total number of NOT '[]' occurrences in the cointelegraph column:\", not_empty_rows.shape[0])\n",
    "# not_empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show an example\n",
    "# row = not_empty_rows['cointelegraph'][45230]\n",
    "# print(f\"Row has {len(row)} news\")\n",
    "# print(f\"List of news: {row}\")\n",
    "# row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of empty rows (equal to '[]') in the cointelegraph column\n",
    "# empty_rows = cointelegraph_hourly_copy[cointelegraph_hourly_copy['cointelegraph'] == '[]']\n",
    "# print(\"Total number of '[]' occurrences in the cointelegraph column:\", empty_rows.shape[0])\n",
    "# empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the timestamp as the index\n",
    "# cointelegraph_hourly_copy = cointelegraph_hourly_copy.set_index('timestamp_begin', drop=False)\n",
    "# cointelegraph_hourly = cointelegraph_hourly_copy\n",
    "# cointelegraph_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Increase the figure size\n",
    "# plt.figure(figsize=(24, 8))\n",
    "\n",
    "# # Plot the number of news items per day\n",
    "# plt.plot(cointelegraph_hourly['timestamp'], cointelegraph_hourly['cointelegraph'].str.count('\\[[^\\]]'))\n",
    "\n",
    "# # Group x axis by days\n",
    "# plt.xticks(cointelegraph_hourly['timestamp'][::24*60], rotation=45)\n",
    "\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "cointelegraph_daily.to_csv(os.path.join(ROOT, NEWS_DATASET_PATH, \"cointelegraph_daily_grouped.csv\"), index=False)\n",
    "\n",
    "# # Save the datasets\n",
    "# cointelegraph_hourly.to_csv(os.path.join(ROOT, NEWS_DATASET_PATH, \"cointelegraph_hourly_grouped.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
