{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ROOT = '../'\n",
    "sys.path.append(ROOT)  # Add the root folder to the sys.path\n",
    "\n",
    "# Import the modules\n",
    "from config import *\n",
    "from models.fasttext import fasttext_lang_model\n",
    "\n",
    "# Reload the configuration\n",
    "from importlib import reload\n",
    "reload(sys.modules['config'])\n",
    "reload(sys.modules['models.fasttext'])\n",
    "\n",
    "# Import the reloaded modules\n",
    "from config import *\n",
    "from models.fasttext import fasttext_lang_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge submissions chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the cleaned submissions datasets and concatenate them into one large dataset\n",
    "SUBMISSIONS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/submissions')\n",
    "SUBMISSIONS_CHUNKS_DIR = os.path.join(SUBMISSIONS_DIR, 'chunks')\n",
    "\n",
    "# Set the chunk size (number of rows to read at a time)\n",
    "chunk_size = 1000000 # rows at a time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Find the rows that contain only urls \n",
    "def find_rows_with_only_urls(chunk):\n",
    "    rows_to_remove = chunk[(chunk['text'].str.startswith('http')) | (chunk['title'].str.startswith('http'))]\n",
    "    print(f'Number of rows that contain only urls: {len(rows_to_remove)}')\n",
    "\n",
    "    # Remove from the dataset the rows in 'rows_to_remove'\n",
    "    chunk = chunk.drop(rows_to_remove.index)\n",
    "\n",
    "    # Reset index\n",
    "    chunk.reset_index(drop=True, inplace=True)\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file where to append chunks\n",
    "# Open the first chunk and save the header\n",
    "chunk_path = os.path.join(SUBMISSIONS_CHUNKS_DIR, os.listdir(SUBMISSIONS_CHUNKS_DIR)[0])\n",
    "submissions = pd.read_csv(chunk_path, nrows=0)\n",
    "submissions.to_parquet(os.path.join(SUBMISSIONS_DIR, 'submissions.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the chunks to the file\n",
    "count_number_of_rows = 0\n",
    "for f in tqdm(os.listdir(SUBMISSIONS_CHUNKS_DIR)):\n",
    "    chunk_path = os.path.join(SUBMISSIONS_CHUNKS_DIR, f)\n",
    "    for chunk in pd.read_csv(chunk_path, chunksize=chunk_size):\n",
    "        if not chunk.empty:\n",
    "            chunk = find_rows_with_only_urls(chunk)\n",
    "            \n",
    "            # Save the chunk to the file\n",
    "            chunk.to_parquet(os.path.join(SUBMISSIONS_DIR, 'submissions.parquet'), mode='a', header=False, index=False)\n",
    "\n",
    "            # Count the number of rows\n",
    "            count_number_of_rows += len(chunk)\n",
    "\n",
    "            # Clean the memory\n",
    "            del chunk\n",
    "\n",
    "# Free memory\n",
    "del submissions \n",
    "\n",
    "print(f'Number of rows in the final dataset: {count_number_of_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter submissions by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSIONS_DIR = os.path.join(ROOT ,SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "SUBMISSIONS_DATASET_NAME = \"submissions.parquet\"\n",
    "dataset_path = os.path.join(SUBMISSIONS_DIR, SUBMISSIONS_DATASET_NAME)\n",
    "\n",
    "submissions = pd.read_parquet(dataset_path)\n",
    "submissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fasttext model\n",
    "fasttext_model = fasttext_lang_model()\n",
    "\n",
    "# Create a new dataset that have the same column as the original dataset but only the rows where the language is 'en'\n",
    "submissions_en = []\n",
    "\n",
    "# For each row in the dataset, predict the language of the title, if the language is not English, predict the language of the text\n",
    "# Select only the rows where the language is 'en'\n",
    "for i, row in tqdm(submissions.iterrows(), total=len(submissions)):    \n",
    "    # Remove \"\\n\" from the text\n",
    "    if(fasttext_model.predict_lang(row['title'].replace(\"\\n\", \" \")) != 'en'):\n",
    "        if(fasttext_model.predict_lang(row['text'].replace(\"\\n\", \" \")) == 'en'):\n",
    "            submissions_en.append(row)\n",
    "    else:\n",
    "        submissions_en.append(row)\n",
    "        \n",
    "print(\"Generating the DataFrame...\")\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "submissions_en = pd.DataFrame(submissions_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "SUBMISSIONS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "SUBMISSIONS_DATASET_NAME = \"submissions_en.parquet\"\n",
    "dataset_path = os.path.join(SUBMISSIONS_DIR, SUBMISSIONS_DATASET_NAME)\n",
    "submissions_en.to_parquet(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSIONS_DIR = os.path.join(ROOT ,SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "SUBMISSIONS_DATASET_NAME = \"submissions.parquet\"\n",
    "dataset_path = os.path.join(SUBMISSIONS_DIR, SUBMISSIONS_DATASET_NAME)\n",
    "\n",
    "submissions = pd.read_parquet(dataset_path)\n",
    "submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSIONS_DIR = os.path.join(ROOT ,SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "SUBMISSIONS_DATASET_NAME = \"submissions_en.parquet\"\n",
    "dataset_path = os.path.join(SUBMISSIONS_DIR, SUBMISSIONS_DATASET_NAME)\n",
    "\n",
    "submissions_en = pd.read_parquet(dataset_path)\n",
    "submissions_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the rows that contains 'comments' as a text in the 'code' column\n",
    "rows_to_fix = submissions[submissions['code'].str.contains('comments', case=False)]\n",
    "rows_to_fix_en = submissions_en[submissions_en['code'].str.contains('comments', case=False)]\n",
    "\n",
    "print(f'Number of rows that contain \"comments\" in the \"code\" column: {len(rows_to_fix)}')\n",
    "print(f'Number of rows that contain \"comments\" in the \"code\" column (en): {len(rows_to_fix_en)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_fix_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: \n",
    "# - Link: https://www.reddit.com/r/paos_comments/comments/3c0p67/comment_id_csr0spc_posted_at_20150703_103921/\n",
    "# - Code: 3c0p67\n",
    "# From 'link' column, extract the 'code' and replace the 'code' column with the extracted code\n",
    "rows_to_fix['code'] = rows_to_fix['link'].str.extract(r'comments/comments/(\\w+)/')\n",
    "rows_to_fix_en['code'] = rows_to_fix_en['link'].str.extract(r'comments/comments/(\\w+)/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_fix_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sobstitute the rows in the original dataset with the fixed rows\n",
    "for i, row in rows_to_fix.iterrows():\n",
    "    submissions.loc[i] = row\n",
    "\n",
    "for i, row in rows_to_fix_en.iterrows():\n",
    "    submissions_en.loc[i] = row\n",
    "\n",
    "# Re-check the rows that contain 'comments' as a text in the 'code' column\n",
    "rows_to_fix = submissions[submissions['code'].str.contains('comments', case=False)]\n",
    "rows_to_fix_en = submissions_en[submissions_en['code'].str.contains('comments', case=False)]\n",
    "\n",
    "print(f'Number of rows that contain \"comments\" in the \"code\" column: {len(rows_to_fix)}')\n",
    "print(f'Number of rows that contain \"comments\" in the \"code\" column (en): {len(rows_to_fix_en)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "SUBMISSIONS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "SUBMISSIONS_DATASET_NAME = \"submissions.parquet\"\n",
    "dataset_path = os.path.join(SUBMISSIONS_DIR, SUBMISSIONS_DATASET_NAME)\n",
    "submissions.to_parquet(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "SUBMISSIONS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "SUBMISSIONS_DATASET_NAME = \"submissions_en.parquet\"\n",
    "dataset_path = os.path.join(SUBMISSIONS_DIR, SUBMISSIONS_DATASET_NAME)\n",
    "submissions_en.to_parquet(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge comments chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submissions dataset\n",
    "SUBMISSIONS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "SUBMISSIONS_DATASET_NAME = 'submissions_en.parquet'\n",
    "submissions = pd.read_parquet(os.path.join(SUBMISSIONS_DIR, SUBMISSIONS_DATASET_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the cleaned comments datasets and concatenate them into one large dataset\n",
    "COMMENTS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/comments/')\n",
    "COMMENTS_CHUNKS_DIR = os.path.join(COMMENTS_DIR, 'chunks/')\n",
    "\n",
    "# Set the chunk size (number of rows to read at a time)\n",
    "chunk_size = 1000000 # rows at a time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Clean 'text' column by removing the rows that contains only urls as a text\n",
    "def find_rows_with_only_urls(chunk):\n",
    "       rows_to_remove = chunk[(chunk['body'].str.startswith('http'))]\n",
    "\n",
    "       # Remove from the dataset the rows in 'rows_to_remove'\n",
    "       chunk = chunk.drop(rows_to_remove.index)\n",
    "\n",
    "       # Reset index\n",
    "       chunk.reset_index(drop=True, inplace=True)\n",
    "       \n",
    "       return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Clean the rows that contains: author, score, created, link, body, code as a text\n",
    "def find_rows_with_author_score_created_link_body_code(chunk):\n",
    "    rows_to_remove = chunk[(chunk['author'].str.contains('author')) | \n",
    "                           (chunk['score'].str.contains('score')) | \n",
    "                           (chunk['created'].str.contains('created')) | \n",
    "                           (chunk['link'].str.contains('link')) | \n",
    "                           (chunk['body'].str.contains('body')) | \n",
    "                           (chunk['code'].str.contains('code'))]\n",
    "                           \n",
    "    print(f'Number of rows that contain: author, score, created, link, body, code as a text: {len(rows_to_remove)}')\n",
    "\n",
    "    # Remove from the dataset the rows in 'rows_to_remove'\n",
    "    chunk = chunk.drop(rows_to_remove.index)\n",
    "\n",
    "    # Reset index\n",
    "    chunk.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file where to append chunks\n",
    "# Open the first chunk and save the header\n",
    "chunk_path = os.path.join(COMMENTS_CHUNKS_DIR, os.listdir(COMMENTS_CHUNKS_DIR)[0])\n",
    "comments = pd.read_csv(chunk_path, nrows=0)\n",
    "# comments.to_csv(os.path.join(COMMENTS_DIR, 'comments.csv'), index=False)\n",
    "comments.to_parquet(os.path.join(COMMENTS_DIR, 'comments.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the chunks to the file\n",
    "count_number_of_rows = 0\n",
    "for f in tqdm(os.listdir(COMMENTS_CHUNKS_DIR)):\n",
    "    chunk_path = os.path.join(COMMENTS_CHUNKS_DIR, f)\n",
    "    for chunk in pd.read_csv(chunk_path, chunksize=chunk_size):\n",
    "        if not chunk.empty:\n",
    "            chunk = find_rows_with_only_urls(chunk)\n",
    "            chunk = find_rows_with_author_score_created_link_body_code(chunk)\n",
    "\n",
    "            # Save the chunk to the file\n",
    "            # chunk.to_csv(os.path.join(COMMENTS_DIR, 'comments.csv'), mode='a', header=False, index=False)\n",
    "            chunk.to_parquet(os.path.join(COMMENTS_DIR, 'comments.parquet'), mode='a', header=False, index=False)\n",
    "\n",
    "            # Count the number of rows\n",
    "            count_number_of_rows += len(chunk)\n",
    "\n",
    "            # Clean the memory\n",
    "            del chunk\n",
    "\n",
    "# Free memory\n",
    "del submissions\n",
    "\n",
    "print(f'Number of rows in the final dataset: {count_number_of_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter comments by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/comments/')\n",
    "COMMENTS_DATASET_NAME = 'comments.parquet'\n",
    "dataset_path = os.path.join(COMMENTS_DIR, COMMENTS_DATASET_NAME)\n",
    "\n",
    "comments = pd.read_parquet(dataset_path)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fasttext model\n",
    "fasttext_model = fasttext_lang_model()\n",
    "\n",
    "# Create a new dataset that have the same column as the original dataset but only the rows where the language is 'en'\n",
    "comments_en = []\n",
    "\n",
    "# For each row in the dataset, predict the language of the title, if the language is not English, predict the language of the text\n",
    "# Select only the rows where the language is 'en'\n",
    "for i, row in tqdm(comments.iterrows(), total=len(comments)):    \n",
    "    # Remove \"\\n\" from the text\n",
    "    if(fasttext_model.predict_lang(row['body'].replace(\"\\n\", \" \")) == 'en'):\n",
    "        comments_en.append(row)\n",
    "        \n",
    "print(\"Generating the DataFrame...\")\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "comments_en = pd.DataFrame(comments_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "COMMENTS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/comments/')\n",
    "COMMENTS_DATASET_NAME = \"comments_en.parquet\"\n",
    "dataset_path = os.path.join(COMMENTS_DIR, COMMENTS_DATASET_NAME)\n",
    "comments_en.to_parquet(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/comments/')\n",
    "COMMENTS_DATASET_NAME = 'comments.parquet'\n",
    "dataset_path = os.path.join(COMMENTS_DIR, COMMENTS_DATASET_NAME)\n",
    "\n",
    "comments = pd.read_parquet(dataset_path)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the rows that contains 'comments' as a text in the 'code' column\n",
    "rows_to_fix = comments[comments['code'].str.contains('comments', case=False)]\n",
    "print(f'Number of rows that contain \"comments\" in the \"code\" column: {len(rows_to_fix)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_fix.iloc[0]['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: \n",
    "# - Link: https://www.reddit.com/r/paos_comments/comments/3c0p67/comment_id_csr0spc_posted_at_20150703_103921/\n",
    "# - Code: 3c0p67\n",
    "# From 'link' column, extract the 'code' and replace the 'code' column with the extracted code\n",
    "rows_to_fix['code'] = rows_to_fix['link'].str.extract(r'comments/comments/(\\w+)/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the indices in rows_to_fix align with those in comments\n",
    "indices = rows_to_fix.index\n",
    "\n",
    "# Update the comments DataFrame using the loc method with all indices at once\n",
    "comments.loc[indices, :] = rows_to_fix.values\n",
    "\n",
    "# Re-check the rows that contain 'comments' as a text in the 'code' column\n",
    "rows_to_fix = comments[comments['code'].str.contains('comments', case=False)]\n",
    "\n",
    "print(f'Number of rows that contain \"comments\" in the \"code\" column: {len(rows_to_fix)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submissions dataset\n",
    "SUBMISSIONS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "SUBMISSIONS_DATASET_NAME = 'submissions_en.parquet'\n",
    "submissions = pd.read_parquet(os.path.join(SUBMISSIONS_DIR, SUBMISSIONS_DATASET_NAME))\n",
    "\n",
    "print(f'Number of rows in the comments dataset: {len(comments)}')\n",
    "\n",
    "# Re-check if the code contained in 'code' column is contained in the 'code' column of the submissions dataset\n",
    "comments = comments[comments['code'].isin(submissions['code'])]\n",
    "\n",
    "print(f'Number of rows in the comments dataset after removing the rows that are not in the submissions dataset: {len(comments)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "COMMENTS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/comments/')\n",
    "COMMENTS_DATASET_NAME = \"comments.parquet\"\n",
    "dataset_path = os.path.join(COMMENTS_DIR, COMMENTS_DATASET_NAME)\n",
    "comments.to_parquet(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comments dataset\n",
    "COMMENTS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/comments/')\n",
    "COMMENTS_DATASET_NAME = 'comments_en.parquet'\n",
    "comments_en = pd.read_parquet(os.path.join(COMMENTS_DIR, COMMENTS_DATASET_NAME))\n",
    "comments_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the rows that contains 'comments' as a text in the 'code' column\n",
    "rows_to_fix = comments_en[comments_en['code'].str.contains('comments', case=False)]\n",
    "print(f'Number of rows that contain \"comments\" in the \"code\" column: {len(rows_to_fix)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_fix.iloc[0]['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: \n",
    "# - Link: https://www.reddit.com/r/paos_comments/comments/3c0p67/comment_id_csr0spc_posted_at_20150703_103921/\n",
    "# - Code: 3c0p67\n",
    "# From 'link' column, extract the 'code' and replace the 'code' column with the extracted code\n",
    "rows_to_fix['code'] = rows_to_fix['link'].str.extract(r'comments/comments/(\\w+)/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the indices in rows_to_fix align with those in comments_en\n",
    "indices = rows_to_fix.index\n",
    "\n",
    "# Update the comments_en DataFrame using the loc method with all indices at once\n",
    "comments_en.loc[indices, :] = rows_to_fix.values\n",
    "\n",
    "# Re-check the rows that contain 'comments' as a text in the 'code' column\n",
    "rows_to_fix = comments_en[comments_en['code'].str.contains('comments', case=False)]\n",
    "\n",
    "print(f'Number of rows that contain \"comments\" in the \"code\" column: {len(rows_to_fix)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submissions dataset\n",
    "SUBMISSIONS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "SUBMISSIONS_DATASET_NAME = 'submissions_en.parquet'\n",
    "submissions = pd.read_parquet(os.path.join(SUBMISSIONS_DIR, SUBMISSIONS_DATASET_NAME))\n",
    "\n",
    "print(f'Number of rows in the comments_en dataset: {len(comments_en)}')\n",
    "\n",
    "# Re-check if the code contained in 'code' column is contained in the 'code' column of the submissions dataset\n",
    "comments_en = comments_en[comments_en['code'].isin(submissions['code'])]\n",
    "\n",
    "print(f'Number of rows in the comments_en dataset after removing the rows that are not in the submissions dataset: {len(comments_en)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "COMMENTS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/comments/')\n",
    "COMMENTS_DATASET_NAME = \"comments_en.parquet\"\n",
    "dataset_path = os.path.join(COMMENTS_DIR, COMMENTS_DATASET_NAME)\n",
    "comments_en.to_parquet(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSIONS_DIR = os.path.join(ROOT ,SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "SUBMISSIONS_DATASET_NAME = \"submissions_en.parquet\"\n",
    "dataset_path = os.path.join(SUBMISSIONS_DIR, SUBMISSIONS_DATASET_NAME)\n",
    "\n",
    "submissions_en = pd.read_parquet(dataset_path)\n",
    "submissions_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/comments/')\n",
    "COMMENTS_DATASET_NAME = 'comments_en.parquet'\n",
    "dataset_path = os.path.join(COMMENTS_DIR, COMMENTS_DATASET_NAME)\n",
    "\n",
    "comments_en = pd.read_parquet(dataset_path)\n",
    "comments_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select submissions that have at least x comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of comments for each submission\n",
    "# Create a dataset with the number of comments for each submission (code)\n",
    "comments_count = comments_en['code'].value_counts().reset_index()\n",
    "comments_count.columns = ['code', 'comments_count']\n",
    "print(f'Number of   submissions that have comments: {len(comments_count)}')\n",
    "\n",
    "# Select the submissions that have x or more comments\n",
    "MIN_COMMENTS = 10\n",
    "submissions_codes = comments_count[comments_count['comments_count'] >= MIN_COMMENTS]\n",
    "print(f'Number of submissions with {MIN_COMMENTS} or more comments: {submissions_codes.shape[0]}')\n",
    "submissions_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From submissions_en, select only the rows where the 'code' column is in submissions_codes\n",
    "submissions_with_at_least_x_comments = submissions_en[submissions_en['code'].isin(submissions_codes['code'])].reset_index(drop=True)\n",
    "submissions_with_at_least_x_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selects submissions that have a score between x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_distribution(score_list, min_score, max_score):\n",
    "    plot_list = [(min_score, 10), (10, 100), (100, 1000), (1000, 10000), (10000, max_score)]\n",
    "    for i, (min_score, max_score) in enumerate(plot_list):\n",
    "        tot_num_submissions = len([score for score in score_list if min_score <= score <= max_score])\n",
    "        plt.figure(figsize=(20, 6))  # Adjust the width and height as desired\n",
    "        plt.hist(score_list, bins=10000)\n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Distribution of Submissions Score [{min_score}, {max_score}] -> {tot_num_submissions} submissions')\n",
    "        plt.xlim([min_score, max_score])\n",
    "        plt.yscale('log')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all the scores into integers\n",
    "submissions_score_list = submissions_with_at_least_x_comments['score'].tolist()\n",
    "submissions_score_list = [int(score) for score in submissions_score_list if type(score) == int or score.isdigit()]\n",
    "print(f\"Min score: {min(submissions_score_list)}, Max score: {max(submissions_score_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_distribution(submissions_score_list, min(submissions_score_list), max(submissions_score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the submissions that have a score from x to y\n",
    "MIN_SCORE = 10\n",
    "MAX_SCORE = max(submissions_score_list)\n",
    "\n",
    "# Convert the scores to integers\n",
    "submissions_with_at_least_x_comments['score'] = pd.to_numeric(submissions_with_at_least_x_comments['score'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Select the submissions that have a score between min_score and max_score\n",
    "submissions_between_scores = submissions_with_at_least_x_comments[\n",
    "    (submissions_with_at_least_x_comments['score'] >= MIN_SCORE) & \n",
    "    (submissions_with_at_least_x_comments['score'] <= MAX_SCORE)\n",
    "    ].reset_index(drop=True)\n",
    "print(f'Number of submissions with a score between {MIN_SCORE} and {MAX_SCORE}: {len(submissions_between_scores)}')\n",
    "submissions_between_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select comments that belong to the filtered submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_comments = comments_en[comments_en['code'].isin(submissions_between_scores['code'])].reset_index(drop=True)\n",
    "filtered_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select comments that have a score between x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert score to integer\n",
    "filtered_comments['score'] = pd.to_numeric(filtered_comments['score'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Convert NAType to 0\n",
    "filtered_comments['score'] = filtered_comments['score'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all the scores into integers\n",
    "comments_score_list = filtered_comments['score'].tolist()\n",
    "comments_score_list = [int(score) for score in comments_score_list if type(score) == int or score.isdigit()]\n",
    "print(f\"Min score: {min(comments_score_list)}, Max score: {max(comments_score_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_distribution(comments_score_list, min(comments_score_list), max(comments_score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the comments that have a score from x to y\n",
    "MIN_SCORE = 10\n",
    "MAX_SCORE = max(comments_score_list)\n",
    "\n",
    "# Select the submissions that have a score between min_score and max_score\n",
    "comments_between_scores = filtered_comments[\n",
    "    (filtered_comments['score'] >= MIN_SCORE) & \n",
    "    (filtered_comments['score'] <= MAX_SCORE)\n",
    "    ].reset_index(drop=True)\n",
    "print(f'Number of comments with a score between {MIN_SCORE} and {MAX_SCORE}: {len(comments_between_scores)}')\n",
    "comments_between_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From submissions_between_scores, select only the rows that have the 'code' in comments_between_scores\n",
    "submissions_filtered = submissions_between_scores[submissions_between_scores['code'].isin(comments_between_scores['code'])].reset_index(drop=True)\n",
    "submissions_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of comments for each submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new empty column \"comments\" to the submissions_between_scores DataFrame\n",
    "submissions_filtered_with_comments_en = submissions_filtered.copy()\n",
    "submissions_filtered_with_comments_en['comments'] = None\n",
    "\n",
    "# For each row in the submissions_filtered_with_comments_en DataFrame, select the comments that have the same 'code' as the current 'code'\n",
    "for i, row in tqdm(submissions_filtered_with_comments_en.iterrows(), total=len(submissions_filtered_with_comments_en)):\n",
    "    code = row['code']\n",
    "    # Select the comments that have the 'code' equal to the current 'code'\n",
    "    comments = comments_between_scores[comments_between_scores['code'] == code]\n",
    "    comments = comments.values.tolist()\n",
    "    submissions_filtered_with_comments_en.at[i, 'comments'] = str(comments)\n",
    "submissions_filtered_with_comments_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "SUBMISSIONS_DIR = os.path.join(ROOT ,SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "output_file = os.path.join(SUBMISSIONS_DIR, 'submissions_filtered_with_comments_en.parquet')\n",
    "submissions_filtered_with_comments_en.to_parquet(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions filtered with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submissions with comments dataset\n",
    "SUBMISSIONS_DIR = os.path.join(ROOT ,SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "output_file = os.path.join(SUBMISSIONS_DIR, 'submissions_filtered_with_comments_en.parquet')\n",
    "submissions_filtered_with_comments_en = pd.read_parquet(output_file)\n",
    "submissions_filtered_with_comments_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nan values\n",
    "print(submissions_filtered_with_comments_en.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the rows with nan values\n",
    "submissions_filtered_with_comments_en[submissions_filtered_with_comments_en.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove row\n",
    "submissions_filtered_with_comments_en.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nan values\n",
    "print(submissions_filtered_with_comments_en.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a CSV file\n",
    "output_file = os.path.join(SUBMISSIONS_DIR, 'submissions_filtered_with_comments_en.parquet')\n",
    "submissions_filtered_with_comments_en.to_parquet(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate daily and hourly reddit dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submissions with comments dataset\n",
    "SUBMISSIONS_DIR = os.path.join(ROOT, SOCIAL_DATASET_PATH, 'reddit/submissions/')\n",
    "output_file = os.path.join(SUBMISSIONS_DIR, 'submissions_filtered_with_comments_en.parquet')\n",
    "submissions_filtered_with_comments_en = pd.read_parquet(output_file)\n",
    "submissions_filtered_with_comments_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the start and end date of the dataset\n",
    "start_date = submissions_filtered_with_comments_en['created'].min().split()[0]\n",
    "end_date = submissions_filtered_with_comments_en['created'].max().split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two temp dataset (daily and hourly) with just the timestamp column that start from START_DATE and end at END_DATE\n",
    "# Daily dataset has the format: 2024-03-16\n",
    "# Hourly dataset has the format: 2024-03-16 20:00:00\n",
    "\n",
    "reddit_daily = pd.date_range(start=start_date, end=end_date, freq='D').to_frame(index=False, name='timestamp')\n",
    "reddit_daily['timestamp'] = reddit_daily['timestamp'].dt.date\n",
    "\n",
    "# For the hourly dataset, generate 2 coluns: timestamp_begin and timestamp_end where timestamp_end is timestamp_begin + 1 hour\n",
    "# reddit_hourly = pd.date_range(start=START_DATE, end=END_DATE, freq='h').to_frame(index=False, name='timestamp')\n",
    "# reddit_hourly['timestamp_begin'] = reddit_hourly['timestamp']\n",
    "# reddit_hourly['timestamp_end'] = reddit_hourly['timestamp'] + pd.Timedelta(hours=1)\n",
    "# reddit_hourly.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# Turn the timestamp column into string\n",
    "reddit_daily['timestamp'] = reddit_daily['timestamp'].astype(str)\n",
    "# reddit_hourly['timestamp_begin'] = reddit_hourly['timestamp_begin'].astype(str)\n",
    "# reddit_hourly['timestamp_end'] = reddit_hourly['timestamp_end'].astype(str)\n",
    "\n",
    "# Add 'reddit' column\n",
    "reddit_daily['reddit'] = None\n",
    "# reddit_hourly['reddit'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reddit_daily\n",
    "reddit_daily_copy = reddit_daily.copy()\n",
    "\n",
    "# Iterate over the daily dataset\n",
    "for index, row in tqdm(reddit_daily_copy.iterrows(), total=len(reddit_daily_copy)):\n",
    "    # Get current timestamp\n",
    "    curr_timestamp = row['timestamp']\n",
    "    # Select the submissions that have been created during the curr_timestamp\n",
    "    # Example: \n",
    "        # if curr_timestamp is 2018-01-01, then select the submission items that have been published from 2018-01-01 00:00:00 to 2018-01-01 23:59:59\n",
    "        # submissions = [submission1, submission2, submission3]\n",
    "    filtered_submissions = submissions_filtered_with_comments_en[submissions_filtered_with_comments_en['created'].str.contains(curr_timestamp)]\n",
    "\n",
    "    # Check if there are any submissions\n",
    "    if len(filtered_submissions) == 0:\n",
    "        reddit_daily_copy.at[index, 'reddit'] = str([]) # Save an empty list\n",
    "    else:\n",
    "        updated_submissions = []\n",
    "        # For each filtered submission, select all the comments that have been created during the curr_timestamp\n",
    "        for i, submission in filtered_submissions.iterrows():\n",
    "            filtered_comments = []\n",
    "            # Select all the comments\n",
    "            comments = ast.literal_eval(submission['comments'])\n",
    "\n",
    "            # Check if there are any comments\n",
    "            if len(comments) != 0:\n",
    "                # Select the comments that have been created during the curr_timestamp, save them as a list\n",
    "                # Example: \n",
    "                    # if curr_timestamp is 2018-01-01, then select the comments items that have been published from 2018-01-01 00:00:00 to 2018-01-01 23:59:59\n",
    "                    # comments = [comment1, comment2, comment3]\n",
    "                for comment in comments:\n",
    "                    if comment[2].startswith(curr_timestamp):\n",
    "                        filtered_comments.append(comment)\n",
    "                if len(filtered_comments) == 0:\n",
    "                    filtered_comments = str([])\n",
    "            else:\n",
    "                filtered_comments = str([])\n",
    "\n",
    "            # Replace the comments list with the new comments list\n",
    "            submission['comments'] = filtered_comments\n",
    "\n",
    "            # Append the submission to the reddit list without columns\n",
    "            updated_submissions.append(list(submission))\n",
    "\n",
    "        # Append the news list to the reddit column\n",
    "        reddit_daily_copy.at[index, 'reddit'] = updated_submissions\n",
    "reddit_daily_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NOT empty rows (different from '[]') in the reddit column\n",
    "not_empty_rows = reddit_daily_copy[reddit_daily_copy['reddit'] != '[]']\n",
    "print(f'Number of rows that are not empty: {len(not_empty_rows)}')\n",
    "not_empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example\n",
    "row = not_empty_rows['reddit'][0] # [x] x: row\n",
    "print(f\"Row has {len(row)} submissions\")\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = row[25]\n",
    "print(f\"Submission has {len(submission[12])} comments\")\n",
    "submission[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of empty rows (equal to '['empty']') in the reddit column\n",
    "empty_rows = reddit_daily_copy[reddit_daily_copy['reddit'] == '[]']\n",
    "print(\"Total number of '[]' occurrences in the reddit column:\", empty_rows.shape[0])\n",
    "empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of '['empty']' in the comment list of each reddit submissions\n",
    "# empty_rows = []\n",
    "# # Select the submissions that don't have '['empty']' in the reddit column\n",
    "# submissions = reddit_daily_copy[reddit_daily_copy['reddit'].str.contains(r'\\[\\'empty\\'\\]') == False]\n",
    "\n",
    "# # Iterate over the submissions\n",
    "# for i, row in submissions.iterrows():\n",
    "#     try:\n",
    "#         submissions = ast.literal_eval(row[\"reddit\"])\n",
    "#         for j, s in enumerate(submissions): \n",
    "#             comments = ast.literal_eval(s[12])\n",
    "#             if comments == \"['empty']\":\n",
    "#                 empty_rows.append(s)\n",
    "#     except:\n",
    "#         print(f'Error at index {i}, {j}')\n",
    "#         print(s)\n",
    "#         print(comments)\n",
    "\n",
    "# empty_rows = pd.DataFrame(empty_rows)\n",
    "# print(\"Total number of submissions without any comments ('['empty']' occurrences in the comment list of each reddit submissions):\", empty_rows.shape[0])\n",
    "# empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the timestamp as the index\n",
    "reddit_daily_copy = reddit_daily_copy.set_index('timestamp', drop=False)\n",
    "reddit_daily = reddit_daily_copy\n",
    "reddit_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate reddit_hourly\n",
    "# reddit_hourly_copy = reddit_hourly.copy()\n",
    "\n",
    "# # Iterate over the hourly dataset\n",
    "# for index, row in tqdm(reddit_hourly_copy.iterrows(), total=len(reddit_hourly_copy)):\n",
    "#     # Get the timestamp\n",
    "#     timestamp_begin = row['timestamp_begin']\n",
    "#     timestamp_end = row['timestamp_end']\n",
    "#     # Select the submission rows items that have been posted between timestamp_begin and timestamp_end, save them as a list\n",
    "#     # Example: if timestamp_begin is 2018-01-01 00:00:00 and timestamp_end is 2018-01-01 01:00:00\n",
    "#     # select the submission items that have been published from 2018-01-01 00:00:00 to 2018-01-01 00:59:59\n",
    "#     # submissions = [submission1, submission2, submission3]\n",
    "#     filtered_submissions = submissions_filtered_with_comments_en[\n",
    "#         (submissions_filtered_with_comments_en['created'] >= timestamp_begin) & \n",
    "#         (submissions_filtered_with_comments_en['created'] < timestamp_end)\n",
    "#     ]\n",
    "    \n",
    "#     # Check if there are any submissions\n",
    "#     if len(filtered_submissions) == 0:\n",
    "#         reddit_hourly_copy.at[index, 'reddit'] = str([])\n",
    "#     else:\n",
    "#         updated_submissions = []\n",
    "#         # For each filtered submission, select all the comments that have been posted between timestamp_begin and timestamp_end\n",
    "#         for i, submission in filtered_submissions.iterrows():\n",
    "#             filtered_comments = []\n",
    "#             # Select all the comments of the submission\n",
    "#             comments = ast.literal_eval(submission['comments'])\n",
    "\n",
    "#             # Check if there are any comments\n",
    "#             if len(comments) != 0:\n",
    "#                 # Select the comments that have been posted between timestamp_begin and timestamp_end, save them as a list\n",
    "#                 # Example: if timestamp_begin is 2018-01-01 00:00:00 and timestamp_end is 2018-01-01 01:00:00\n",
    "#                 # select the comments that have been published from 2018-01-01 00:00:00 to 2018-01-01 00:59:59\n",
    "#                 # comments = [comment1, comment2, comment3]\n",
    "#                 for comment in comments:\n",
    "#                     if comment[2] >= timestamp_begin and comment[2] < timestamp_end:\n",
    "#                         filtered_comments.append(comment)\n",
    "#                 if len(filtered_comments) == 0:\n",
    "#                     filtered_comments = str([])\n",
    "#             else:\n",
    "#                 filtered_comments = str([])\n",
    "\n",
    "#             # Replace the comments list with the new comments list\n",
    "#             submission['comments'] = filtered_comments\n",
    "\n",
    "#             # Append the submission to the reddit list without columns\n",
    "#             updated_submissions.append(list(submission))\n",
    "\n",
    "#         # Append the news list to the reddit column\n",
    "#         reddit_hourly_copy.at[index, 'reddit'] = updated_submissions\n",
    "# reddit_hourly_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of NOT empty rows(different from '[]') in the cointredditelegraph column\n",
    "# not_empty_rows = reddit_hourly_copy[reddit_hourly_copy['reddit'] != '[]']\n",
    "# print(\"Total number of NOT '[]' occurrences in the reddit column:\", not_empty_rows.shape[0])\n",
    "# not_empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show an example\n",
    "# row = not_empty_rows['reddit'][5] # [x] x: row\n",
    "# print(f\"Row has {len(row)} submissions\")\n",
    "# row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = row[0]\n",
    "# print(f\"Submission has {len(submission[12])} comments\")\n",
    "# submission[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of empty rows (equal to '[]') in the reddit column\n",
    "# empty_rows = reddit_hourly_copy[reddit_hourly_copy['reddit'] == '[]']\n",
    "# print(\"Total number of '[]' occurrences in the reddit column:\", empty_rows.shape[0])\n",
    "# empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of '['empty']' in the comment list of each reddit submissions\n",
    "# empty_rows = []\n",
    "# # Select the submissions that don't have '['empty']' in the reddit column\n",
    "# submissions = reddit_hourly_copy[reddit_hourly_copy['reddit'].str.contains(r'\\[\\'empty\\'\\]') == False]\n",
    "\n",
    "# # Iterate over the submissions\n",
    "# for i, row in submissions.iterrows():\n",
    "#     try:\n",
    "#         submissions = ast.literal_eval(row[\"reddit\"])\n",
    "#         for j, s in enumerate(submissions): \n",
    "#             comments = ast.literal_eval(s[12])\n",
    "#             if comments == \"['empty']\":\n",
    "#                 empty_rows.append(s)\n",
    "#     except:\n",
    "#         print(f'Error at index {i}, {j}')\n",
    "#         print(s)\n",
    "#         print(comments)\n",
    "\n",
    "# empty_rows = pd.DataFrame(empty_rows)\n",
    "# print(\"Total number of submissions without any comments ('['empty']' occurrences in the comment list of each reddit submissions):\", empty_rows.shape[0])\n",
    "# empty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the timestamp as the index\n",
    "# reddit_hourly_copy = reddit_hourly_copy.set_index('timestamp_begin', drop=False)\n",
    "# reddit_hourly = reddit_hourly_copy\n",
    "# reddit_hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "REDDIT_DIR = os.path.join(ROOT ,SOCIAL_DATASET_PATH, 'reddit')\n",
    "reddit_daily.to_csv(os.path.join(REDDIT_DIR, \"reddit_daily_grouped.csv\"), index=False)\n",
    "\n",
    "# Save the datasets\n",
    "# REDDIT_DIR = os.path.join(ROOT ,SOCIAL_DATASET_PATH, 'reddit')\n",
    "# reddit_hourly.to_csv(os.path.join(REDDIT_DIR, \"reddit_hourly_grouped.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
